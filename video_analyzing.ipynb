{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b7c30ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 150\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "NUM_FEATURES = 2048\n",
    "MAX_SEQ_LENGTH = 200\n",
    "\n",
    "video_dir = 'Cattle_Diseases/*/*.mp4'\n",
    "\n",
    "class_dict = {\n",
    "            'Bovine Spongiform Encephalopathy' : 0,\n",
    "            'Healthy' : 1,\n",
    "            'Heat Stress' : 2,\n",
    "            'Lameness' : 3,\n",
    "            'Unknown' : 4,\n",
    "            }\n",
    "class_dict_rev = {v: k for k, v in class_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "11ba82c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2, glob, os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eb76dbaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    248.000000\n",
      "mean      77.185484\n",
      "std       65.671415\n",
      "min       11.000000\n",
      "25%       33.750000\n",
      "50%       49.000000\n",
      "75%       97.000000\n",
      "max      358.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def frame_length_distribution():\n",
    "    frame_lengths = []\n",
    "    for video_path in glob.glob(video_dir):\n",
    "        vidcap = cv2.VideoCapture(video_path)\n",
    "        frame_length = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT)) // 5\n",
    "        frame_lengths.append(frame_length)\n",
    "        \n",
    "    frame_dist = pd.Series(frame_lengths)\n",
    "    frame_dist.hist()\n",
    "    plt.xlabel('Frame Length')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.savefig('visualization/frame length.png')\n",
    "    plt.show()\n",
    "    \n",
    "    print(frame_dist.describe())\n",
    "\n",
    "\n",
    "frame_length_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "153f7f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_center_square(frame):\n",
    "    y, x = frame.shape[0:2]\n",
    "    min_dim = min(y, x)\n",
    "    start_x = (x // 2) - (min_dim // 2)\n",
    "    start_y = (y // 2) - (min_dim // 2)\n",
    "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n",
    "\n",
    "\n",
    "def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    try:\n",
    "        frame_count = 0\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if frame_count % 5 == 0:\n",
    "                if not ret:\n",
    "                    break\n",
    "                frame = crop_center_square(frame)\n",
    "                frame = cv2.resize(frame, resize)\n",
    "                frame = frame[:, :, [2, 1, 0]]\n",
    "                frames.append(frame)\n",
    "\n",
    "                if len(frames) == max_frames:\n",
    "                    break\n",
    "\n",
    "            frame_count += 1\n",
    "    finally:\n",
    "        cap.release()\n",
    "    return np.array(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8dd409ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_extractor():\n",
    "    feature_extractor = tf.keras.applications.InceptionV3(\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False,\n",
    "        pooling=\"avg\",\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    )\n",
    "    preprocess_input = tf.keras.applications.inception_v3.preprocess_input\n",
    "\n",
    "    inputs = tf.keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
    "    preprocessed = preprocess_input(inputs)\n",
    "\n",
    "    outputs = feature_extractor(preprocessed)\n",
    "    return tf.keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
    "\n",
    "\n",
    "feature_extractor = build_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d3301a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_all_videos():\n",
    "    video_paths = glob.glob(video_dir)\n",
    "    num_samples = len(video_paths)\n",
    "\n",
    "    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype=\"bool\")\n",
    "    frame_features = np.zeros(\n",
    "        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "    )\n",
    "    labels = []\n",
    "    for idx, path in enumerate(video_paths):\n",
    "        label = path.split('\\\\')[1]\n",
    "        labels.append(class_dict[label])\n",
    "\n",
    "        # Gather all its frames and add a batch dimension.\n",
    "        frames = load_video(path)\n",
    "        frames = frames[None, ...]\n",
    "\n",
    "        temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "        temp_frame_features = np.zeros(\n",
    "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "        )\n",
    "\n",
    "        # Extract features from the frames of the current video.\n",
    "        for i, batch in enumerate(frames):\n",
    "            video_length = batch.shape[0]\n",
    "            length = min(MAX_SEQ_LENGTH, video_length)\n",
    "            for j in range(length):\n",
    "                temp_frame_features[i, j, :] = feature_extractor.predict(\n",
    "                    batch[None, j, :]\n",
    "                )\n",
    "            temp_frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
    "\n",
    "        frame_features[idx,] = temp_frame_features.squeeze()\n",
    "        frame_masks[idx,] = temp_frame_mask.squeeze()\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    return (frame_features, frame_masks), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4263f02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame features in data set: (248, 200, 2048)\n",
      "Frame masks in data set: (248, 200)\n",
      "Epoch 1/150\n",
      "16/16 [==============================] - 6s 118ms/step - loss: 1.7697 - accuracy: 0.1532\n",
      "Epoch 2/150\n",
      "16/16 [==============================] - 2s 117ms/step - loss: 1.6535 - accuracy: 0.1935\n",
      "Epoch 3/150\n",
      "16/16 [==============================] - 2s 118ms/step - loss: 1.5995 - accuracy: 0.2621\n",
      "Epoch 4/150\n",
      "16/16 [==============================] - 2s 117ms/step - loss: 1.5477 - accuracy: 0.2944\n",
      "Epoch 5/150\n",
      "16/16 [==============================] - 2s 116ms/step - loss: 1.4884 - accuracy: 0.3347\n",
      "Epoch 6/150\n",
      "16/16 [==============================] - 2s 127ms/step - loss: 1.4320 - accuracy: 0.3871\n",
      "Epoch 7/150\n",
      "16/16 [==============================] - 2s 116ms/step - loss: 1.3814 - accuracy: 0.4113\n",
      "Epoch 8/150\n",
      "16/16 [==============================] - 2s 116ms/step - loss: 1.3175 - accuracy: 0.4395\n",
      "Epoch 9/150\n",
      "16/16 [==============================] - 2s 115ms/step - loss: 1.3070 - accuracy: 0.4315\n",
      "Epoch 10/150\n",
      "16/16 [==============================] - 2s 117ms/step - loss: 1.2418 - accuracy: 0.4677\n",
      "Epoch 11/150\n",
      "16/16 [==============================] - 2s 120ms/step - loss: 1.1709 - accuracy: 0.4879\n",
      "Epoch 12/150\n",
      "16/16 [==============================] - 2s 122ms/step - loss: 1.1723 - accuracy: 0.4677\n",
      "Epoch 13/150\n",
      "16/16 [==============================] - 2s 120ms/step - loss: 1.1180 - accuracy: 0.5282\n",
      "Epoch 14/150\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 1.1097 - accuracy: 0.5242\n",
      "Epoch 15/150\n",
      "16/16 [==============================] - 2s 120ms/step - loss: 1.0538 - accuracy: 0.6129\n",
      "Epoch 16/150\n",
      "16/16 [==============================] - 2s 120ms/step - loss: 1.0275 - accuracy: 0.6452\n",
      "Epoch 17/150\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 0.9673 - accuracy: 0.7419\n",
      "Epoch 18/150\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 0.9749 - accuracy: 0.7339\n",
      "Epoch 19/150\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 0.9068 - accuracy: 0.7540\n",
      "Epoch 20/150\n",
      "16/16 [==============================] - 2s 118ms/step - loss: 0.8532 - accuracy: 0.7823\n",
      "Epoch 21/150\n",
      "16/16 [==============================] - 2s 120ms/step - loss: 0.8425 - accuracy: 0.7702\n",
      "Epoch 22/150\n",
      "16/16 [==============================] - 2s 120ms/step - loss: 0.8306 - accuracy: 0.7661\n",
      "Epoch 23/150\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 0.7623 - accuracy: 0.7823\n",
      "Epoch 24/150\n",
      "16/16 [==============================] - 2s 118ms/step - loss: 0.7814 - accuracy: 0.7944\n",
      "Epoch 25/150\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 0.7343 - accuracy: 0.7742\n",
      "Epoch 26/150\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 0.6865 - accuracy: 0.7823\n",
      "Epoch 27/150\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 0.6558 - accuracy: 0.7944\n",
      "Epoch 28/150\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 0.6556 - accuracy: 0.8145\n",
      "Epoch 29/150\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 0.6381 - accuracy: 0.8145\n",
      "Epoch 30/150\n",
      "16/16 [==============================] - 2s 120ms/step - loss: 0.6287 - accuracy: 0.8185\n",
      "Epoch 31/150\n",
      "16/16 [==============================] - 2s 120ms/step - loss: 0.6244 - accuracy: 0.8065\n",
      "Epoch 32/150\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 0.5386 - accuracy: 0.8145\n",
      "Epoch 33/150\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 0.5786 - accuracy: 0.8024\n",
      "Epoch 34/150\n",
      "16/16 [==============================] - 2s 118ms/step - loss: 0.6658 - accuracy: 0.7742\n",
      "Epoch 35/150\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 0.5711 - accuracy: 0.8266\n",
      "Epoch 36/150\n",
      "16/16 [==============================] - 2s 120ms/step - loss: 0.5432 - accuracy: 0.8145\n",
      "Epoch 37/150\n",
      "16/16 [==============================] - 2s 120ms/step - loss: 0.5359 - accuracy: 0.8065\n",
      "Epoch 38/150\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 0.4847 - accuracy: 0.8306\n",
      "Epoch 39/150\n",
      "16/16 [==============================] - 2s 120ms/step - loss: 0.4873 - accuracy: 0.8266\n",
      "Epoch 40/150\n",
      "16/16 [==============================] - 2s 118ms/step - loss: 0.4997 - accuracy: 0.8266\n",
      "Epoch 41/150\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 0.4759 - accuracy: 0.8266\n",
      "Epoch 42/150\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 0.4297 - accuracy: 0.8468\n",
      "Epoch 43/150\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 0.4430 - accuracy: 0.8508\n",
      "Epoch 44/150\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 0.4579 - accuracy: 0.8266\n",
      "Epoch 45/150\n",
      "16/16 [==============================] - 2s 118ms/step - loss: 0.4401 - accuracy: 0.8629\n",
      "Epoch 46/150\n",
      "16/16 [==============================] - 2s 117ms/step - loss: 0.4016 - accuracy: 0.8669\n",
      "Epoch 47/150\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 0.4304 - accuracy: 0.8629\n",
      "Epoch 48/150\n",
      "16/16 [==============================] - 2s 117ms/step - loss: 0.4180 - accuracy: 0.8750\n",
      "Epoch 49/150\n",
      "16/16 [==============================] - 2s 120ms/step - loss: 0.3758 - accuracy: 0.8831\n",
      "Epoch 50/150\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 0.3836 - accuracy: 0.8669\n",
      "Epoch 51/150\n",
      "16/16 [==============================] - 2s 118ms/step - loss: 0.3561 - accuracy: 0.8831\n",
      "Epoch 52/150\n",
      "16/16 [==============================] - 2s 120ms/step - loss: 0.3331 - accuracy: 0.9113\n",
      "Epoch 53/150\n",
      "16/16 [==============================] - 2s 120ms/step - loss: 0.3564 - accuracy: 0.8871\n",
      "Epoch 54/150\n",
      "16/16 [==============================] - 2s 120ms/step - loss: 0.3508 - accuracy: 0.8911\n",
      "Epoch 55/150\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 0.3082 - accuracy: 0.9274\n",
      "Epoch 56/150\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 0.3107 - accuracy: 0.9113\n",
      "Epoch 57/150\n",
      "16/16 [==============================] - 2s 118ms/step - loss: 0.3157 - accuracy: 0.9113\n",
      "Epoch 58/150\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 0.2877 - accuracy: 0.9234\n",
      "Epoch 59/150\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 0.2864 - accuracy: 0.9153\n",
      "Epoch 60/150\n",
      "16/16 [==============================] - 2s 118ms/step - loss: 0.2916 - accuracy: 0.9073\n",
      "Epoch 61/150\n",
      "16/16 [==============================] - 2s 120ms/step - loss: 0.2971 - accuracy: 0.9234\n",
      "Epoch 62/150\n",
      "16/16 [==============================] - 2s 120ms/step - loss: 0.2887 - accuracy: 0.9315\n",
      "Epoch 63/150\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 0.2431 - accuracy: 0.9597\n",
      "Epoch 64/150\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 0.2520 - accuracy: 0.9234\n",
      "Epoch 65/150\n",
      "16/16 [==============================] - 2s 120ms/step - loss: 0.2548 - accuracy: 0.9274\n",
      "Epoch 66/150\n",
      "16/16 [==============================] - 2s 118ms/step - loss: 0.2668 - accuracy: 0.9274\n",
      "Epoch 67/150\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 0.2285 - accuracy: 0.9315\n",
      "Epoch 68/150\n",
      "16/16 [==============================] - 2s 120ms/step - loss: 0.2294 - accuracy: 0.9274\n",
      "Epoch 69/150\n",
      "16/16 [==============================] - 2s 120ms/step - loss: 0.2444 - accuracy: 0.9355\n",
      "Epoch 70/150\n",
      "16/16 [==============================] - 2s 120ms/step - loss: 0.2111 - accuracy: 0.9435\n",
      "Epoch 71/150\n",
      "16/16 [==============================] - 2s 120ms/step - loss: 0.2271 - accuracy: 0.9476\n",
      "Epoch 72/150\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 0.2214 - accuracy: 0.9516\n",
      "Epoch 73/150\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 0.1899 - accuracy: 0.9435\n",
      "Epoch 74/150\n",
      "16/16 [==============================] - 2s 121ms/step - loss: 0.2002 - accuracy: 0.9153\n",
      "Epoch 75/150\n",
      "16/16 [==============================] - 2s 123ms/step - loss: 0.1928 - accuracy: 0.9435\n",
      "Epoch 76/150\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 0.2254 - accuracy: 0.9234\n",
      "Epoch 77/150\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 0.2195 - accuracy: 0.9435\n",
      "Epoch 78/150\n",
      "16/16 [==============================] - 2s 120ms/step - loss: 0.1859 - accuracy: 0.9597\n",
      "Epoch 79/150\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 0.1910 - accuracy: 0.9476\n",
      "Epoch 80/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 2s 119ms/step - loss: 0.1858 - accuracy: 0.9516\n",
      "Epoch 81/150\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 0.1869 - accuracy: 0.9355\n",
      "Epoch 82/150\n",
      "16/16 [==============================] - 2s 118ms/step - loss: 0.1513 - accuracy: 0.9597\n",
      "Epoch 83/150\n",
      "16/16 [==============================] - 2s 118ms/step - loss: 0.1489 - accuracy: 0.9597\n",
      "Epoch 84/150\n",
      "16/16 [==============================] - 2s 120ms/step - loss: 0.1565 - accuracy: 0.9476\n",
      "Epoch 85/150\n",
      "16/16 [==============================] - 2s 120ms/step - loss: 0.1563 - accuracy: 0.9476\n",
      "Epoch 86/150\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 0.1924 - accuracy: 0.9194\n",
      "Epoch 87/150\n",
      "16/16 [==============================] - 2s 118ms/step - loss: 0.1266 - accuracy: 0.9597\n",
      "Epoch 88/150\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 0.1445 - accuracy: 0.9556\n",
      "Epoch 89/150\n",
      "16/16 [==============================] - 2s 120ms/step - loss: 0.1371 - accuracy: 0.9677\n",
      "Epoch 90/150\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 0.1323 - accuracy: 0.9597\n",
      "Epoch 91/150\n",
      "16/16 [==============================] - 2s 118ms/step - loss: 0.1344 - accuracy: 0.9556\n",
      "Epoch 92/150\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 0.1391 - accuracy: 0.9476\n",
      "Epoch 93/150\n",
      "16/16 [==============================] - 2s 121ms/step - loss: 0.1251 - accuracy: 0.9677\n",
      "Epoch 94/150\n",
      "16/16 [==============================] - 2s 120ms/step - loss: 0.1488 - accuracy: 0.9516\n",
      "Epoch 95/150\n",
      "16/16 [==============================] - 2s 118ms/step - loss: 0.1467 - accuracy: 0.9597\n",
      "Epoch 96/150\n",
      "16/16 [==============================] - 2s 117ms/step - loss: 0.1225 - accuracy: 0.9597\n",
      "Epoch 97/150\n",
      "16/16 [==============================] - 2s 120ms/step - loss: 0.1360 - accuracy: 0.9516\n",
      "Epoch 98/150\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 0.1259 - accuracy: 0.9637\n",
      "Epoch 99/150\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 0.1247 - accuracy: 0.9476\n",
      "Epoch 100/150\n",
      "16/16 [==============================] - 2s 118ms/step - loss: 0.1610 - accuracy: 0.9355\n",
      "Epoch 101/150\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 0.1063 - accuracy: 0.9798\n",
      "Epoch 102/150\n",
      "16/16 [==============================] - 2s 118ms/step - loss: 0.1480 - accuracy: 0.9597\n",
      "Epoch 103/150\n",
      "16/16 [==============================] - 2s 118ms/step - loss: 0.1416 - accuracy: 0.9516\n",
      "Epoch 104/150\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 0.1370 - accuracy: 0.9556\n",
      "Epoch 105/150\n",
      "16/16 [==============================] - 2s 121ms/step - loss: 0.1611 - accuracy: 0.9597\n",
      "Epoch 106/150\n",
      "16/16 [==============================] - 2s 120ms/step - loss: 0.1748 - accuracy: 0.9315\n",
      "Epoch 107/150\n",
      "16/16 [==============================] - 2s 118ms/step - loss: 0.1682 - accuracy: 0.9516\n",
      "Epoch 108/150\n",
      "16/16 [==============================] - 2s 118ms/step - loss: 0.1492 - accuracy: 0.9677\n",
      "Epoch 109/150\n",
      "16/16 [==============================] - 2s 119ms/step - loss: 0.2125 - accuracy: 0.9274\n"
     ]
    }
   ],
   "source": [
    "def get_sequence_model():\n",
    "    frame_features_input = tf.keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))\n",
    "    mask_input = tf.keras.Input((MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "\n",
    "    x = tf.keras.layers.GRU(16, return_sequences=True)(\n",
    "                                                        frame_features_input, \n",
    "                                                        mask=mask_input\n",
    "                                                        )\n",
    "    x = tf.keras.layers.GRU(8)(x)\n",
    "    x = tf.keras.layers.Dropout(0.4)(x)\n",
    "    x = tf.keras.layers.Dense(8, activation=\"relu\")(x)\n",
    "    output = tf.keras.layers.Dense(len(class_dict), activation=\"softmax\")(x)\n",
    "\n",
    "    rnn_model = tf.keras.Model([frame_features_input, mask_input], output)\n",
    "\n",
    "    rnn_model.compile(\n",
    "                    loss=\"sparse_categorical_crossentropy\", \n",
    "                    optimizer=\"adam\", \n",
    "                    metrics=[\"accuracy\"]\n",
    "                    )\n",
    "    return rnn_model\n",
    "\n",
    "\n",
    "def run_experiment():\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "                                                    monitor=\"loss\", \n",
    "                                                    patience=8, \n",
    "                                                    restore_best_weights=True\n",
    "                                                    )\n",
    "\n",
    "    seq_model = get_sequence_model()\n",
    "    history = seq_model.fit(\n",
    "                        [data[0], data[1]],\n",
    "                        labels,\n",
    "                        epochs=EPOCHS,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        callbacks=[early_stopping],\n",
    "                        )\n",
    "\n",
    "    seq_model.save('weights/behaviour_classification.h5')\n",
    "    return history, seq_model\n",
    "\n",
    "if not os.path.exists('weights/behaviour_classification.h5'):\n",
    "    data, labels = prepare_all_videos()\n",
    "\n",
    "    print(f\"Frame features in data set: {data[0].shape}\")\n",
    "    print(f\"Frame masks in data set: {data[1].shape}\")\n",
    "    history, sequence_model = run_experiment()\n",
    "else:\n",
    "    sequence_model = tf.keras.models.load_model('weights/behaviour_classification.h5')\n",
    "    sequence_model.compile(\n",
    "                            loss=\"sparse_categorical_crossentropy\", \n",
    "                            optimizer=\"adam\", \n",
    "                            metrics=[\"accuracy\"]\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1720e1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_9 (InputLayer)           [(None, 200, 2048)]  0           []                               \n",
      "                                                                                                  \n",
      " input_10 (InputLayer)          [(None, 200)]        0           []                               \n",
      "                                                                                                  \n",
      " gru_2 (GRU)                    (None, 200, 16)      99168       ['input_9[0][0]',                \n",
      "                                                                  'input_10[0][0]']               \n",
      "                                                                                                  \n",
      " gru_3 (GRU)                    (None, 8)            624         ['gru_2[0][0]']                  \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 8)            0           ['gru_3[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 8)            72          ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 5)            45          ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 99,909\n",
      "Trainable params: 99,909\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sequence_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574d4bf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "73fac52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_video(path):\n",
    "    frames = load_video(path)\n",
    "    frames = frames[None, ...]\n",
    "\n",
    "    temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "    temp_frame_features = np.zeros(\n",
    "        shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "    )\n",
    "\n",
    "    # Extract features from the frames of the current video.\n",
    "    for i, batch in enumerate(frames):\n",
    "        video_length = batch.shape[0]\n",
    "        length = min(MAX_SEQ_LENGTH, video_length)\n",
    "        for j in range(length):\n",
    "            temp_frame_features[i, j, :] = feature_extractor.predict(\n",
    "                batch[None, j, :]\n",
    "            )\n",
    "        temp_frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
    "    return (temp_frame_features.squeeze(), temp_frame_mask.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "847149f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_classification_inference(path):\n",
    "    data = process_single_video(path)\n",
    "    frame_features = data[0][None, ...]\n",
    "    frame_masks = data[1][None, ...]\n",
    "    predictions = sequence_model.predict([frame_features, frame_masks])\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "    return class_dict_rev[predictions[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b39921fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bovine Spongiform Encephalopathy'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'Cattle_Diseases/Bovine Spongiform Encephalopathy/aggressive1.mp4'\n",
    "video_classification_inference(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "13fe0155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Unknown'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'Cattle_Diseases/Unknown/bird3.mp4'\n",
    "video_classification_inference(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "97a76923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Healthy'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'Cattle_Diseases/Healthy/healthy_walking4.mp4'\n",
    "video_classification_inference(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4b1dc83c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lameness'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'Cattle_Diseases/Lameness/lameness_1.mp4'\n",
    "video_classification_inference(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b551e3c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lameness'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'Cattle_Diseases/Healthy/healthy_eating.mp4'\n",
    "video_classification_inference(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa030599",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "f544ce1a915a9875fad91c894e2c0bcad4b7a79945aa6027ef3ad27810072aa6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
